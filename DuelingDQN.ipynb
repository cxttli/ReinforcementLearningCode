{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import collections\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ENV = \"CartPole-v0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MEMORY_SIZE = 1000\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "PERIOD_PRMTS = 100\n",
    "PERIOD_EPSILON = 800\n",
    "\n",
    "EPISODE = 10000\n",
    "MAX_STEP = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AgentModel():\n",
    "    def __init__(self, env , sess , dueling = True , out_graph = False , out_dqn = True):\n",
    "        self.epsilon = 0.8\n",
    "        self.gamma = 0.5\n",
    "        self.action_dim = env.action_space.n\n",
    "        self.state_dim = env.observation_space.shape[0]\n",
    "        self.l1_units = 20\n",
    "        self.learning_rate = 0.001\n",
    "        self.dueling = dueling\n",
    "        self.out_dqn = out_dqn\n",
    "        self.sess = sess\n",
    "        self.loss_his = []\n",
    "        self.network()  \n",
    "        if out_graph:\n",
    "            tf.summary.FileWriter(\"DuelingDQN/summaries\") \n",
    "\n",
    "    # net_frame\n",
    "    def net_frame(self, scope , clt_name , inputs ):\n",
    "        weights_init = tf.random_normal_initializer()\n",
    "        biases_init = tf.constant_initializer(0.1)\n",
    "        \n",
    "        with tf.variable_scope(\"layer1\"):\n",
    "            weights1 = tf.get_variable(\"weights\" , shape = [self.state_dim , self.l1_units],\n",
    "                                       initializer = weights_init , collections = clt_name)\n",
    "            biases1 = tf.get_variable(\"biases\" , shape = [self.l1_units] , initializer = biases_init , \n",
    "                                     collections = clt_name)\n",
    "            wx_b = tf.matmul(inputs, weights1 ) + biases1\n",
    "            h1 = tf.nn.relu(wx_b)\n",
    "\n",
    "        if self.dueling == True :\n",
    "            with tf.variable_scope(\"value_stream\"):\n",
    "                weights_v = tf.get_variable(\"weights\" , shape = [self.l1_units , 1] , \n",
    "                                            initializer = weights_init , collections = clt_name )\n",
    "                biases_v = tf.get_variable(\"biases\" , shape = [1] , initializer = biases_init , \n",
    "                                           collections = clt_name )\n",
    "                value = tf.matmul(h1 , weights_v) + biases_v\n",
    "\n",
    "            with tf.variable_scope(\"advantage_stream\"):\n",
    "                weights_a = tf.get_variable(\"weights\" , shape = [self.l1_units , self.action_dim] , \n",
    "                                          initializer = weights_init , collections = clt_name )\n",
    "                biases_a = tf.get_variable(\"biases\" , shape = [self.action_dim ] , \n",
    "                                          initializer = biases_init , collections = clt_name )\n",
    "                advantage = tf.matmul(h1 , weights_a) + biases_a\n",
    "\n",
    "            with tf.variable_scope(\"aggregating_moudle\"):\n",
    "                q_out = value + advantage - tf.reduce_mean(advantage , axis = 1 , keep_dims = True )  # ***keep_dims\n",
    "\n",
    "        elif self.out_dqn:\n",
    "            with tf.variable_scope(\"layer2\"):\n",
    "                weights2 = tf.get_variable(\"weights\" , shape = [self.l1_units , self.action_dim] , \n",
    "                                           initializer = weights_init , collections = clt_name)\n",
    "                biases2 = tf.get_variable(\"biases\" , shape = [self.action_dim] , \n",
    "                                          initializer = biases_init , collections = clt_name )\n",
    "                q_out = tf.matmul( h1 , weights2 ) + biases2\n",
    "\n",
    "        return q_out\n",
    "        \n",
    "    # network\n",
    "    def network(self):\n",
    "        # creat q_network\n",
    "        self.inputs_q = tf.placeholder(dtype = tf.float32 , shape = [None , self.state_dim])\n",
    "        clt_name_q = [\"q_network_prmts\", tf.GraphKeys.GLOBAL_VARIABLES ]\n",
    "        scope_q = \"Q_network\"\n",
    "        with tf.variable_scope(\"Q_network\"):\n",
    "            self.q_value = self.net_frame( scope_q , clt_name_q , self.inputs_q )\n",
    "        \n",
    "        #cteate q_target \n",
    "        self.inputs_target = tf.placeholder(dtype = tf.float32 , shape = [None , self.state_dim])\n",
    "        clt_name_target = [\"target_network_prmts\" , tf.GraphKeys.GLOBAL_VARIABLES]\n",
    "        scope_target = \"target_network\"\n",
    "        with tf.variable_scope(\"target_network\"):\n",
    "            self.q_target = self.net_frame( scope_target , clt_name_target , self.inputs_target )\n",
    "        \n",
    "        with tf.variable_scope(\"loss\"):\n",
    "            self.target_train = tf.placeholder(dtype = tf.float32 , shape = [None , self.action_dim])\n",
    "            self.loss = tf.reduce_mean( tf.squared_difference(self.target_train , self.q_value) )\n",
    "            \n",
    "        with tf.variable_scope(\"train\"):\n",
    "            self.train_op = tf.train.RMSPropOptimizer(self.learning_rate).minimize(self.loss)\n",
    "            \n",
    "    # training\n",
    "    def train(self, state , action , reward , next_state):\n",
    "        # solve target\n",
    "        q_value , q_target = self.sess.run([self.q_value , self.q_target] , \n",
    "                                           feed_dict={self.inputs_q : state , self.inputs_target:next_state})\n",
    "        q_approx =  reward + self.gamma * np.argmax(q_target , axis=1 )\n",
    "        target = q_value\n",
    "        batch_index = np.arange(BATCH_SIZE)\n",
    "        target[batch_index , action ] = q_approx\n",
    "        # training\n",
    "        loss , _ = self.sess.run([self.loss , self.train_op] , \n",
    "                                 feed_dict = { self.inputs_q : state , self.target_train : target })\n",
    "        self.loss_his.append(loss)\n",
    "        \n",
    "    # chose action\n",
    "    def chose_action(self , current_state):\n",
    "        state = current_state[np.newaxis , :]\n",
    "        q_value_action = self.sess.run( self.q_value , feed_dict={self.inputs_q : state})\n",
    "        \n",
    "        if np.random.random() > self.epsilon:\n",
    "            action_chosen = np.random.choice(self.action_dim) #***[0]***\n",
    "        else: \n",
    "            action_chosen = np.argmax(q_value_action)\n",
    "        \n",
    "        return action_chosen\n",
    "            \n",
    "    # decay epsilon\n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon = self.epsilon - 0.1\n",
    "        \n",
    "    # update prmts\n",
    "    def update_prmts(self):\n",
    "        q_prmts = tf.get_collection(\"q_network_prmts\")\n",
    "        t_prmts = tf.get_collection(\"target_network_prmts\")\n",
    "        self.sess.run( [tf.assign(t,q) for t ,q in zip(t_prmts , q_prmts) ])\n",
    "        print(\"updating prmts...\")\n",
    "    \n",
    "    def greedy_action(self , current_state):\n",
    "        current_state = current_state[np.newaxis , :]  \n",
    "        q = self.sess.run(self.q_value , feed_dict={self.inputs_q : current_state} ) \n",
    "        action_greedy = np.argmax(q)\n",
    "        return action_greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "memory = []\n",
    "Transition = collections.namedtuple(\"Transition\", [\"state\" , \"action\" , \"reward\" , \"next_state\" , \"done\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train( agent , env ):\n",
    "    reward_his = []\n",
    "    all_reward = 0\n",
    "    step_his = []\n",
    "    update_iter = 0\n",
    "    for episode in range(EPISODE):\n",
    "        state = env.reset()\n",
    "        for step in range(MAX_STEP):\n",
    "            action = agent.chose_action(state)\n",
    "            next_state , reward , done , _ = env.step(action)\n",
    "            all_reward += reward\n",
    "                    \n",
    "            if len(memory) == MEMORY_SIZE:\n",
    "                memory.pop(0)\n",
    "            memory.append(Transition(state , action , reward , next_state , done))\n",
    "            \n",
    "            if len(memory) > BATCH_SIZE:\n",
    "                batch_transition = random.sample(memory , BATCH_SIZE)\n",
    "                batch_state , batch_action , batch_reward , batch_next_state , batch_done = map(np.array , zip(*batch_transition))\n",
    "                agent.train(batch_state , batch_action , batch_reward , batch_next_state)\n",
    "                update_iter += 1\n",
    "                \n",
    "                if update_iter % PERIOD_PRMTS == 0:\n",
    "                    agent.update_prmts()\n",
    "                \n",
    "            if done:\n",
    "                step_his.append(step)\n",
    "                reward_his.append(all_reward)\n",
    "                break\n",
    "                \n",
    "            state = next_state\n",
    "    loss_his = agent.loss_his\n",
    "    return [step_his , reward_his , loss_his]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env = gym.make( ENV )\n",
    "    sess = tf.Session()\n",
    "    with tf.variable_scope(\"DQN\"):\n",
    "        DQN = AgentModel(env , sess , dueling = False , out_graph = False , out_dqn = True)\n",
    "    with tf.variable_scope(\"DuelingDQN\"):\n",
    "        DuelingDQN = AgentModel(env , sess , dueling = True , out_graph = True )\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    step_dqn , reward_dqn , loss_dqn = train(DQN , env)\n",
    "    step_dueling , reward_dueling , loss_dueling = train( DuelingDQN , env)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
