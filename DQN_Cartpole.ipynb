{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import collections\n",
    "import gym\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ENV = \"CartPole-v0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MEMORY_SIZE = 1000\n",
    "EPISODES = 2000\n",
    "MAX_STEP = 500\n",
    "BATCH_SIZE = 32\n",
    "UPDATE_PERIOD = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##built class for the DQN\n",
    "class DeepQNetwork():\n",
    "    def __init__(self , env , sess=None , gamma = 0.8, epsilon = 0.8 ):\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.action_dim = env.action_space.n\n",
    "        self.state_dim = env.observation_space.shape[0]\n",
    "        self.network()\n",
    "        self.sess = sess\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        tf.summary.FileWriter(\"DQN/summaries\" , sess.graph )\n",
    "        \n",
    "    # net_frame using for creating Q & target network\n",
    "    def net_frame(self , scope , collections_name , inputs):\n",
    "        weights_init = tf.truncated_normal_initializer(0 , 0.3)\n",
    "        bias_init = tf.constant_initializer(0.1)\n",
    "        \n",
    "        with tf.variable_scope(scope):\n",
    "            with tf.variable_scope(\"layer1\"):\n",
    "                weights1 = tf.get_variable(name = \"weights\" , dtype = tf.float32 , shape = [self.state_dim , 64] ,\n",
    "                                           initializer = weights_init , collections = collections_name )\n",
    "                bias1 = tf.get_variable(name = \"bias\" , dtype = tf.float32 , shape = [64] , initializer = bias_init ,\n",
    "                                         collections = collections_name)\n",
    "                wx_b = tf.matmul(self.inputs_q , weights1) + bias1 \n",
    "                h1 = tf.nn.relu( wx_b )\n",
    "                \n",
    "            with tf.variable_scope(\"layer2\"):\n",
    "                weights2 = tf.get_variable(name = \"weights\" , dtype = tf.float32 , shape = [64 , 32] ,\n",
    "                                           initializer = weights_init , collections = collections_name )\n",
    "                bias2 = tf.get_variable(name = \"bias\" , dtype = tf.float32 , shape = [32] , initializer = bias_init ,\n",
    "                                         collections = collections_name)\n",
    "                wx_b = tf.matmul( h1 , weights2) + bias2 \n",
    "                h2 = tf.nn.relu( wx_b )\n",
    "                \n",
    "            with tf.variable_scope(\"layer3\"):\n",
    "                weights3 = tf.get_variable(name = \"weights\" , dtype = tf.float32 , shape = [32 , self.action_dim] , \n",
    "                                           initializer = weights_init , collections = collections_name )\n",
    "                bias3 = tf.get_variable(name = \"bias\" , dtype = tf.float32 , shape = [self.action_dim], \n",
    "                                        initializer = bias_init , collections = collections_name)\n",
    "                q_out = tf.matmul(h2 , weights3 ) + bias3 \n",
    "                \n",
    "            return q_out\n",
    "        \n",
    "    # create q_network & target_network     \n",
    "    def network(self):       \n",
    "        # q_network\n",
    "        self.inputs_q = tf.placeholder(dtype = tf.float32 , shape = [None , self.state_dim] , name = \"inputs_q\")\n",
    "        scope_var = \"q_network\" \n",
    "        clt_name_var = [\"q_net_prmt\" , tf.GraphKeys.GLOBAL_VARIABLES]    \n",
    "        self.q_value = self.net_frame(scope_var , clt_name_var , self.inputs_q )\n",
    "            \n",
    "        # target_network\n",
    "        self.inputs_target = tf.placeholder(dtype = tf.float32 , shape = [None , self.state_dim] , name = \"inputs_target\")\n",
    "        scope_tar = \"target_network\" \n",
    "        clt_name_tar = [\"target_net_prmt\" , tf.GraphKeys.GLOBAL_VARIABLES]    \n",
    "        self.q_target = self.net_frame(scope_tar , clt_name_tar , self.inputs_target )\n",
    "               \n",
    "        with tf.variable_scope(\"loss\"):\n",
    "            self.target = tf.placeholder(dtype = tf.float32 , shape = [None , self.action_dim] , name = \"target\")\n",
    "            self.loss = tf.reduce_mean( tf.square(self.q_value - self.target))\n",
    "\n",
    "        with tf.variable_scope(\"train\"):\n",
    "            self.train_op = tf.train.RMSPropOptimizer(0.01).minimize(self.loss)    \n",
    "    \n",
    "    # training\n",
    "    def train(self , state , reward , action , state_next):\n",
    "        q , q_target = self.sess.run([self.q_value , self.q_target] , \n",
    "                                     feed_dict={self.inputs_q : state , self.inputs_target : state_next } )\n",
    "        target = reward + self.gamma * np.max(q_target , axis = 1)\n",
    "\n",
    "        self.reform_target = q.copy()\n",
    "        batch_index = np.arange(BATCH_SIZE , dtype = np.int32)\n",
    "        self.reform_target[batch_index , action] = target\n",
    "    \n",
    "        loss , _ = self.sess.run([self.loss , self.train_op] , feed_dict={self.inputs_q: state , self.target: self.reform_target} )\n",
    "    \n",
    "    # chose action\n",
    "    def chose_action(self , current_state):\n",
    "        current_state = current_state[np.newaxis , :]  #*** array dim: (xx,)  --> (1 , xx) ***\n",
    "        q = self.sess.run(self.q_value , feed_dict={self.inputs_q : current_state} )\n",
    "        \n",
    "        # e-greedy\n",
    "        if np.random.random() < self.epsilon:\n",
    "            action_chosen = np.random.randint(0 , self.action_dim)\n",
    "        else:\n",
    "            action_chosen = np.argmax(q)\n",
    "        \n",
    "        return action_chosen\n",
    "         \n",
    "    #upadate parmerters\n",
    "    def update_prmt(self):\n",
    "        q_prmts = tf.get_collection(\"q_net_prmt\")\n",
    "        target_prmts = tf.get_collection(\"target_net_prmt\")\n",
    "        self.sess.run( [tf.assign(t , q)for t,q in zip(target_prmts , q_prmts)])  #***\n",
    "        print(\"updating target-network parmeters...\")\n",
    "        \n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon = self.epsilon - 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# memory for momery replay\n",
    "memory = []\n",
    "Transition = collections.namedtuple(\"Transition\" , [\"state\", \"action\" , \"reward\" , \"next_state\" , \"done\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(ENV)\n",
    "    with tf.Session() as sess:\n",
    "        DQN = DeepQNetwork(env , sess )\n",
    "        update_iter = 0\n",
    "        step_his = []\n",
    "        for episode in range(EPISODES):\n",
    "            state = env.reset()\n",
    "            env.render() \n",
    "            reward_all = 0\n",
    "            #training\n",
    "            for step in range(MAX_STEP):\n",
    "                action = DQN.chose_action(state)\n",
    "                next_state , reward , done , _ = env.step(action)\n",
    "                reward_all += reward \n",
    "\n",
    "                if len(memory) > MEMORY_SIZE:\n",
    "                    memory.pop(0)\n",
    "                memory.append(Transition(state, action , reward , next_state , done))\n",
    "\n",
    "                if len(memory) > BATCH_SIZE:\n",
    "                    batch_transition = random.sample(memory , BATCH_SIZE)\n",
    "                    #***\n",
    "                    batch_state, batch_action, batch_reward, batch_next_state, batch_done = map(np.array , zip(*batch_transition))  \n",
    "                    DQN.train(state = batch_state ,\n",
    "                              reward = batch_reward , \n",
    "                              action = batch_action , \n",
    "                              state_next = batch_next_state\n",
    "                             )\n",
    "                    update_iter += 1\n",
    "\n",
    "                if update_iter % UPDATE_PERIOD == 0:\n",
    "                    DQN.update_prmt()\n",
    "                \n",
    "                if update_iter % 200 == 0:\n",
    "                    DQN.decay_epsilon()\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "                    \n",
    "                state = next_state\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
